I">n<hr />
<p><strong>Note</strong></p>

<p>This is <strong>Part II</strong> of a three-part series.</p>

<ul>
  <li>Part I - use case, pattern concepts</li>
  <li>Part II - naïve implementation</li>
  <li>Part III - pattern implementation</li>
</ul>

<hr />

<h2 id="introduction">Introduction</h2>

<p>Let’s quickly review Part I of the series.</p>

<p>In Part I, we:</p>
<ul>
  <li>Made the case for extensibility as a quality attribute to seek in designing algorithm-centric production workflows.</li>
  <li>Presented a fictional use case for a containerized analysis worker environment, a common kind of setup in real-world applications, that reads images from a queue and runs algorithms on them.</li>
  <li>Came up with an initial design the worker, naïve with respect to extensibility.</li>
  <li>Analyzed what made the initial design less extensible and used our conclusions to come up with a design pattern that solves for extensibility.</li>
</ul>

<p>In Part II, we’ll go through implementing the naïve design. This is useful mostly as a precursor to Part III where we’ll re-implement our worker using the final design pattern. By first having the initial design coded and functioning, we’ll be able to apply the pattern to it, test that it still works and look at how much easier it is to extend in practical terms.</p>

<h3 id="a-note-on-implementation-and-code-structure">A note on implementation and code structure</h3>

<p>As explained in Part I, we’ll implement everything in Python. I include some code snippets throughout the article tailored to aid discussion, but you can find the complete working code for the example <a href="https://github.com/shaiperson/worker-pattern-article">this GitHub repo</a>. Code for the initial design presented here is in branch <code class="language-plaintext highlighter-rouge">initial</code>.</p>

<p>The code for all componentes is placed in a single repository. Looking at the repo, you’ll find a directory for each component with its source files, Dockerfile and a <code class="language-plaintext highlighter-rouge">build.sh </code> script. The <code class="language-plaintext highlighter-rouge">worker</code> and <code class="language-plaintext highlighter-rouge">producer</code> directories are there to assist in running and testing everything locally.</p>

<h2 id="implementation">Implementation</h2>

<p>To quickly review Part I, the initial design we’ll implement here consists of a Controller component that reads images from a queue and sends them to a Runner component. The latter houses the actual algorithm code, and exposes it on an auxiliary HTTP server for the controller to send requests to.</p>

<p><img src="/assets/plug-play-worker-pattern-part-1/Untitled.png" style="display: block; margin-left: auto; margin-right: auto; width: 30%;" /></p>

<h3 id="controller">Controller</h3>

<p>We’ll be using a RabbitMQ queue called <code class="language-plaintext highlighter-rouge">tasks</code> in our example setup, so we set up our controller with <code class="language-plaintext highlighter-rouge">pika</code> to consume from the <code class="language-plaintext highlighter-rouge">tasks</code> queue.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pika</span>

<span class="n">connection</span> <span class="o">=</span> <span class="n">pika</span><span class="p">.</span><span class="n">BlockingConnection</span><span class="p">(</span><span class="n">pika</span><span class="p">.</span><span class="n">ConnectionParameters</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s">'localhost'</span><span class="p">))</span>
<span class="n">channel</span> <span class="o">=</span> <span class="n">connection</span><span class="p">.</span><span class="n">channel</span><span class="p">()</span>
<span class="n">channel</span><span class="p">.</span><span class="n">queue_declare</span><span class="p">(</span><span class="n">queue</span><span class="o">=</span><span class="s">'tasks'</span><span class="p">)</span>
</code></pre></div></div>

<p>We define a callback to be run on each consumed message. This callback does the controller work: call the runner, check for errors in the response and do something with the result or the error. In our example, we just log the result if the image was processed successfully and the error otherwise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>

<span class="n">RUNNER_HOST</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'RUNNER_HOST'</span><span class="p">,</span> <span class="s">'localhost'</span><span class="p">)</span>
<span class="n">RUNNER_PORT</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'RUNNER_PORT'</span><span class="p">,</span> <span class="mi">5000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">body</span><span class="p">):</span>
		<span class="k">print</span><span class="p">(</span><span class="s">'Received message, calling runner'</span><span class="p">)</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Content-Type'</span><span class="p">:</span> <span class="s">'application/json'</span><span class="p">}</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s">'http://{}:{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">RUNNER_HOST</span><span class="p">,</span> <span class="n">RUNNER_PORT</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">request</span><span class="p">(</span><span class="s">"POST"</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">ok</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Received result from runner: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Received error response from runner: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
        <span class="c1"># Handle error (retry/requeue/send to dead-letter exchange)
</span></code></pre></div></div>

<p>Lastly, we start listening for messages on the queue.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">channel</span><span class="p">.</span><span class="n">basic_consume</span><span class="p">(</span><span class="n">queue</span><span class="o">=</span><span class="n">QUEUE_NAME</span><span class="p">,</span> <span class="n">on_message_callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">auto_ack</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Listening for messages on queue </span><span class="si">{</span><span class="n">QUEUE_NAME</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">channel</span><span class="p">.</span><span class="n">start_consuming</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="classifier">Classifier</h3>

<p>I trained a simple classifier for the purposes of this article. Since our focus here is on the ML Ops side of things, we’ll just use the model as a black box and look at how to use and deploy it. However, you can check out the supported memes, model, data and full code for training in this <a href="https://www.kaggle.com/shaibianchi/meme-classifier/">kaggle notebook</a>. Credit to Keras’s <a href="https://keras.io/guides/transfer_learning/">transfer learning guide</a> and to <a href="https://www.kaggle.com/gmorinan/memes-classified-and-labelled">gmor’s meme dataset on Kaggle</a>.</p>

<p>We define a <code class="language-plaintext highlighter-rouge">classifier</code> module responsible for loading and running the classifier. In this module, the model is loaded and compiled from its <code class="language-plaintext highlighter-rouge">.h5</code> serialization upon initialization. In this case, we assume the <code class="language-plaintext highlighter-rouge">.h5</code> file is packaged with the code in a <code class="language-plaintext highlighter-rouge">model/</code> directory at root level.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'model/meme-classifier-model.json'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">model_from_json</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>

<span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'model/meme-classifier-model.h5'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span>
    <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span>
<span class="p">)</span>
</code></pre></div></div>

<p>For running the model, we expose a <code class="language-plaintext highlighter-rouge">run_on_url</code> function. It uses some auxiliary functions to process the input image and the output. We’ll omit the code for these functions to reduce cluttering here, but you can find it in the repo.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_on_url</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">'Fetching image'</span><span class="p">)</span>
    <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">_get_image_bytes</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

    <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">'Reading and preparing image'</span><span class="p">)</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">_get_image_tensor</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">)</span>

    <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">'Running on image'</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_pred_to_label</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</code></pre></div></div>

<p>We also define a <code class="language-plaintext highlighter-rouge">server</code> module responsible for exposing the classifier on a local HTTP endpoint. We use the Python <code class="language-plaintext highlighter-rouge">http</code> module to set up a simple local HTTP server that calls <code class="language-plaintext highlighter-rouge">classifier.run_on_url</code> upon <code class="language-plaintext highlighter-rouge">POST</code> requests on <code class="language-plaintext highlighter-rouge">/</code>  with a JSON body containing an image URL.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ClassifierServer</span><span class="p">(</span><span class="n">BaseHTTPRequestHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_set_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">content_type</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">send_response</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">send_header</span><span class="p">(</span><span class="s">'Content-type'</span><span class="p">,</span> <span class="n">content_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">end_headers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">do_POST</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Process request
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="n">content_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">headers</span><span class="p">[</span><span class="s">'Content-Length'</span><span class="p">])</span>
            <span class="n">post_data</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rfile</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">content_length</span><span class="p">)</span>
            <span class="n">post_json</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">post_data</span><span class="p">)</span>
            <span class="n">image_url</span> <span class="o">=</span> <span class="n">post_json</span><span class="p">[</span><span class="s">'url'</span><span class="p">]</span>

        <span class="k">except</span> <span class="nb">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="p">.</span><span class="n">error</span><span class="p">(</span><span class="s">'ValueError while parsing body as JSON'</span><span class="p">)</span>
            <span class="n">response_body</span> <span class="o">=</span> <span class="p">{</span><span class="s">'message'</span><span class="p">:</span> <span class="sa">f</span><span class="s">'Task body not valid JSON: "</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"'</span><span class="p">}</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_set_response</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="s">'application/json'</span><span class="p">)</span>

        <span class="k">except</span> <span class="nb">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">response_body</span> <span class="o">=</span> <span class="p">{</span><span class="s">'message'</span><span class="p">:</span> <span class="sa">f</span><span class="s">'Task body missing field </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">'</span><span class="p">}</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_set_response</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="s">'application/json'</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">logger</span><span class="p">.</span><span class="n">debug</span><span class="p">(</span><span class="s">'Running algorithm on URL {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">image_url</span><span class="p">))</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">run_on_url</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
                <span class="n">response_body</span> <span class="o">=</span> <span class="p">{</span><span class="s">'result'</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_response</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="s">'application/json'</span><span class="p">)</span>

            <span class="k">except</span> <span class="n">exceptions</span><span class="p">.</span><span class="n">RequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">response_body</span> <span class="o">=</span> <span class="p">{</span><span class="s">'message'</span><span class="p">:</span> <span class="sa">f</span><span class="s">'Error fetching request image, received </span><span class="si">{</span><span class="n">e</span><span class="p">.</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="s">'</span><span class="p">}</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_response</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="p">,</span> <span class="s">'application/json'</span><span class="p">)</span>

            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">response_body</span> <span class="o">=</span> <span class="p">{</span><span class="s">'message'</span><span class="p">:</span> <span class="s">'Internal error'</span><span class="p">,</span> <span class="s">'error'</span><span class="p">:</span> <span class="n">error_str</span><span class="p">}</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">_set_response</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="s">'application/json'</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">wfile</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">response_body</span><span class="p">).</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
</code></pre></div></div>

<p>Notice the error management. As we saw above, the controller looks out for an error status being returned by its runner and does some management of the failure to process a given message (like logging it and/or sending it to a dead-letter exchange). So in implementing the runner, we want to be diligent in capturing expected errors so that we can return meaningful error status for the controller to handle, which will later help us quickly understand failed messages and be robust in managing unexpected errors.</p>

<h3 id="dockerization-compose-file">Dockerization, Compose File</h3>

<p>I tend to favor containerizing all componentes that go into a worker setup as this lends itself very well to deployment using orchestration tools such as AWS ECS or Kubernetes. It also helps in minimizing differences between development, staging and production environments. We’ll use Docker Compose to run our setup locally.</p>

<h4 id="controller-dockerfile">Controller Dockerfile</h4>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.7</span>

<span class="k">WORKDIR</span><span class="s"> /opt/project</span>

<span class="k">COPY</span><span class="s"> ./* ./</span>

<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<h4 id="classifier-dockerfile">Classifier Dockerfile</h4>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> tensorflow/tensorflow</span>

<span class="k">WORKDIR</span><span class="s"> /opt/project</span>

<span class="k">RUN </span>pip <span class="nb">install </span>pillow

<span class="k">COPY</span><span class="s"> ./* ./</span>
<span class="k">COPY</span><span class="s"> ./model ./model</span>
</code></pre></div></div>

<h4 id="compose-file">Compose file</h4>

<p>We set our environment up with the queue service and both components of our worker. I only show some relevant fields from the file, but you can check it out with all of its details in the repo.</p>

<p>The <code class="language-plaintext highlighter-rouge">restart</code> and <code class="language-plaintext highlighter-rouge">depends_on</code> clauses in the <code class="language-plaintext highlighter-rouge">controller</code> service are there to allow a warmup period for the <code class="language-plaintext highlighter-rouge">rabbitmq</code> service after it starts.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">rabbitmq</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">rabbitmq:3</span> 
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">5672:5672</span>
      
  <span class="na">meme-classifier</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">meme-classifier</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">python server.py</span>
    
  <span class="na">controller</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">controller</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">python main.py</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">on-failure</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">rabbitmq</span>
</code></pre></div></div>

<h4 id="producer">Producer</h4>

<p>The code in the <code class="language-plaintext highlighter-rouge">producer</code> directory is there to aid in testing. It’s set up to connect to the queue and allow us to easily send some test messages for our worker to process.</p>

<h3 id="testing-it-out">Testing it out</h3>

<p>After building the component images and tagging them appropriately, run <code class="language-plaintext highlighter-rouge">docker-compose up -d</code> in the <code class="language-plaintext highlighter-rouge">worker </code> directory to spin up the environment. Run <code class="language-plaintext highlighter-rouge">docker-compse logs -f</code> to track initialization. You’re likely to see some connection errors from <code class="language-plaintext highlighter-rouge">controller</code> as it fails to connect to <code class="language-plaintext highlighter-rouge">queue</code> while the latter completes its initialization.</p>

<p>Once both the controller and classifier services are listening for messages and requests respectively, we can send some meme image URLs to the queue and get some classification happening.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
meme-classifier | INFO :: [+] Listening on port 5000
...
controller      | INFO :: [+] Listening for messages on queue tasks
</code></pre></div></div>

<p>Let’s try one out.</p>

<p><img src="https://memegenerator.net/img/instances/39673831.jpg" alt="What if I told you / Matrix Morpheus - what if i told you i have no idea how my glasses dont fall out" /></p>

<p>From a terminal at <code class="language-plaintext highlighter-rouge">./producer/</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python main.py <span class="s2">"https://memegenerator.net/img/instances/39673831.jpg"</span>
</code></pre></div></div>

<p>Logs:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>controller         | INFO :: Received message, calling runner
meme-classifier    | INFO :: Running classifier on URL
controller         | INFO :: Received result from runner: {'label': 'matrix_morpheus', 'score': 0.99989}
</code></pre></div></div>

<p>Looking good! You can play around some more with it if you like and have some fun looking at memes as I have while doing the same 😛.</p>

<h2 id="whats-next">What’s Next</h2>

<p>In Part III of the series, we’ll implement the final pattern presented in Part I and test it. We’ll then try to further extend our resulting setup with a new algorithm and see in practical terms if we achieved our goal of making it low-overhead and easy to do.</p>
:ET