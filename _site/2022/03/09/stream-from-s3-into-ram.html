<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ML on AWS Lambda: Two Practices | Blog</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="ML on AWS Lambda: Two Practices" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="/2022/03/09/stream-from-s3-into-ram.html" />
<meta property="og:url" content="/2022/03/09/stream-from-s3-into-ram.html" />
<meta property="og:site_name" content="Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-09T12:55:00-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ML on AWS Lambda: Two Practices" />
<script type="application/ld+json">
{"dateModified":"2022-03-09T12:55:00-03:00","datePublished":"2022-03-09T12:55:00-03:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2022/03/09/stream-from-s3-into-ram.html"},"url":"/2022/03/09/stream-from-s3-into-ram.html","description":"Introduction","headline":"ML on AWS Lambda: Two Practices","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">ML on AWS Lambda: Two Practices</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-03-09T12:55:00-03:00" itemprop="datePublished">Mar 9, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>I’d like to share a pattern that emerged in my work deploying machine learning (or ML) inference Python code to AWS Lambda. To do this, first I’ll offer three observations. Then, we’ll derive two practices from those observations that you might want to consider for your own processes.</p>

<h2 id="observations">Observations</h2>

<h3 id="1-on-lambda-and-its-filesystem-feature">1. On Lambda and its filesystem feature</h3>
<p>AWS Lambda can be a convenient way of deploying code to production for many use cases. Naturally, however, it also comes with limitations. In particular, its serverless and flexible nature implies a limitation regarding local storage: each Lambda execution environment gets a filesystem on an ephemeral storage space for it to use which is available under <code class="language-plaintext highlighter-rouge">/tmp</code> and has a hard limit of 512 MB in size (see <a href="https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html">docs</a>).</p>

<h3 id="2-on-using-popular-ml-libraries">2. On using popular ML libraries</h3>

<p>Popular ML libraries that offer pre-trained models (such as <a href="https://huggingface.co/models">Hugging Face</a>, <a href="https://github.com/openai/CLIP">OpenAI’s CLIP</a> or <a href="https://github.com/JaidedAI/EasyOCR">JaidedAI’s EasyOCR</a>) commonly use your filesystem to download models to and to use as cache. Further, these libraries may differ in the default filesystem paths they’d use as cache, but they usually expose a way to configure the paths they’ll use. On the other hand, as far as I can tell at this point in time, they don’t usually offer a way to stream models into RAM. Using your filesystem as a means for caching models is convenient for one’s process since it allows for a smoother iteration on code development, but it also means you’re forced to rely on a filesystem, and one with sufficient available space, to be able to get your models downloaded.</p>

<p>Also quite common is for single models or the conjunction of several models used in an inference program to exceed 512 MB.</p>

<h3 id="3-on-downloading-objects-from-s3">3. On downloading objects from S3</h3>

<p>When you need to get an object from S3, you often just need to download it to disk. This seems intuitive and easy, and is a common practice. You may even do this when you only need the object in RAM, by downloading it first to your filesystem and then loading it into your program from there:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">boto3</span>
<span class="n">s3_resource</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">resource</span><span class="p">(</span><span class="s">'s3'</span><span class="p">)</span>

<span class="n">s3_object</span> <span class="o">=</span> <span class="n">s3_resource</span><span class="p">.</span><span class="n">Object</span><span class="p">(</span><span class="s">'my-bucket'</span><span class="p">,</span> <span class="s">'my_file.txt'</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'temp_my_file.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">s3_object</span><span class="p">.</span><span class="n">download_fileobj</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># do something with temp_my_file.txt, remember to delete it if appropriate.
</span></code></pre></div></div>

<p>However, if you’re going to download the S3 object to disk only to then load and use it in RAM, then you’d probably prefer a way to get the object into RAM directly.</p>

<h2 id="deriving-practices">Deriving practices</h2>

<p>Deploying ML inference code is a worthy use case for Lambda. But, in light of popular libraries relying on disk space for getting you pre-trained models, if you use enough of these then your lambdas are liable to eventually run into the 512 MB limit. This would be the case even if you don’t download pre-trained models but you do rely on getting models (either pre-downloaded, trained by you or whatever the case may be) from a remote storage service with which you’re interacting in a similar way to what I showed before with S3.</p>

<p>So, to solve this potential and likely issue, two good practices you may want to adopt follow.</p>

<h3 id="1-make-libraries-on-disk-cache-path-configurable-by-environment">1. Make libraries’ on-disk cache path configurable by environment</h3>

<p>Either on a model-by-model basis or for all of your library-downloaded models, leverage these libraries’ options to configure cache paths and use environment variables to set them. If you work with a separate ML team that writes your Lambda-bound code, encourage them to adopt this practice on your behalf. That way, when you deploy your code to Lambda, you’ll be able to easily have those paths stem from <code class="language-plaintext highlighter-rouge">/tmp</code> and to avoid “read-only filesystem” errors coming from your libraries attempting to write to off-limits paths. This won’t save you from running into the storage space limit, but it will make deployment easier while your space usage is within bounds.</p>

<p>To view this in an example, let’s assume our code uses CLIP’s <code class="language-plaintext highlighter-rouge">ViT-B/32</code> and Hugging Face’s <code class="language-plaintext highlighter-rouge">bert-base-cased</code>. This means our code at a certain point might include some lines like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">clip</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span>

<span class="n">model</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">clip</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'ViT-B/32'</span><span class="p">)</span>
<span class="n">bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'bert-base-cased'</span><span class="p">)</span>
</code></pre></div></div>

<p>The default cache dirs used by <code class="language-plaintext highlighter-rouge">clip</code> and <code class="language-plaintext highlighter-rouge">transformers</code> in these function calls are (at this point in time) under <code class="language-plaintext highlighter-rouge">~/.cache</code>, namely <code class="language-plaintext highlighter-rouge">~/.cache/torch/transformers</code> and <code class="language-plaintext highlighter-rouge">~/.cache/clip</code> respectively. To adopt this practice, you’d set something like a <code class="language-plaintext highlighter-rouge">MODEL_CACHE_FS_PATH</code> environment variable to a path starting with <code class="language-plaintext highlighter-rouge">/tmp</code> and use those libraries’ cache path configuration options:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">clip</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span>

<span class="n">configured_cache_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MODEL_CACHE_FS_PATH'</span><span class="p">,</span> <span class="s">'./cache'</span><span class="p">)</span>

<span class="n">model</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">clip</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'ViT-B/32'</span><span class="p">,</span> <span class="n">download_root</span><span class="o">=</span><span class="n">configured_cache_path</span><span class="p">)</span>
<span class="n">bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">'bert-base-cased'</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">configured_cache_path</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="use-s3-to-store-models-and-stream-them-into-ram">Use S3 to store models and stream them into RAM</h3>

<p>Once your library-downloaded models (or other large files you may need) conspire to exceed the 512 MB to your lambdas, you’ll need a way to download them that does not require a filesystem. S3 is indeed a good option, since it does offer an easy way to stream objects directly to RAM. If you’re deploying Python code to Lambda, this is very easy to do using <code class="language-plaintext highlighter-rouge">boto3</code>. The way you might take advantage of this option is to pre-download the models you’ve been getting through your libraries’ API, serialize them (e.g by using <code class="language-plaintext highlighter-rouge">pickle</code> or library-specific serialization APIs), upload the serialized files to S3 and then fetch those objects in your Lambda-bound code in a way that gets them into RAM directly. Note, of course, that this is useful for models you get from other sources as well, such as the ones you train yourself.</p>

<p>To quickly look at an example for this, let’s assume we’re using PyTorch and you’ve pickled and uploaded the <code class="language-plaintext highlighter-rouge">state_dict</code> of an instance of <code class="language-plaintext highlighter-rouge">Model</code> to S3 at <code class="language-plaintext highlighter-rouge">models-bucket/model_state_dict.pkl</code>. This is what your code might look like.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">import</span> <span class="nn">boto3</span>
<span class="n">s3_resource</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">resource</span><span class="p">(</span><span class="s">'s3'</span><span class="p">)</span>

<span class="c1"># Stream pickled `state_dict` into variable rather than save to disk
</span><span class="n">bytes_stream</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">()</span>
<span class="n">s3_object</span> <span class="o">=</span> <span class="n">s3_resource</span><span class="p">.</span><span class="n">Object</span><span class="p">(</span><span class="s">'models-bucket'</span><span class="p">,</span> <span class="s">'model_state_dict.pkl'</span><span class="p">)</span>
<span class="n">s3_object</span><span class="p">.</span><span class="n">download_fileobj</span><span class="p">(</span><span class="n">bytes_stream</span><span class="p">)</span>
<span class="n">pickled_state_dict</span> <span class="o">=</span> <span class="n">bytes_stream</span><span class="p">.</span><span class="n">getvalue</span><span class="p">()</span>

<span class="c1"># Load model
</span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">pickled_state_dict</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Have fun with model
</span></code></pre></div></div>

<h3 id="a-final-note">A final note</h3>

<p>Streaming objects from S3 into RAM is nothing new nor is it too much of an obscure functionality if you look at the docs. However, it seems worth highlighting in the particular context of deploying ML code to Lambda. This is an increasingly popular go-to for ML deployment, and these practices easily solve an issue that anyone starting to adopt Lambda for ML is bound to encounter.</p>


  </div><a class="u-url" href="/2022/03/09/stream-from-s3-into-ram.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

<!--    <h2 class="footer-heading">Blog</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blog</li><li><a class="u-email" href="mailto:bianchishai@gmail.com">bianchishai@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/shaiperson"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">shaiperson</span></a></li><li><a href="https://www.linkedin.com/in/shaibianchi"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">shaibianchi</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>MS in Computer Science, freelance engineer and entrepreneur</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
