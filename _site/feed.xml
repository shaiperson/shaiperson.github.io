<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-03-02T23:34:55-03:00</updated><id>/feed.xml</id><title type="html">Blog</title><subtitle>MS in Computer Science, freelance engineer and entrepreneur</subtitle><entry><title type="html">Plug &amp;amp; Play Worker Pattern - Part I</title><link href="/2022/02/15/plug-play-worker-pattern-1.html" rel="alternate" type="text/html" title="Plug &amp;amp; Play Worker Pattern - Part I" /><published>2022-02-15T12:55:00-03:00</published><updated>2022-02-15T12:55:00-03:00</updated><id>/2022/02/15/plug-play-worker-pattern-1</id><content type="html" xml:base="/2022/02/15/plug-play-worker-pattern-1.html">&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;Part I&lt;/strong&gt; of a three-part series.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Part I - use case, pattern concepts&lt;/li&gt;
  &lt;li&gt;Part II - naïve implementation&lt;/li&gt;
  &lt;li&gt;Part III - pattern implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In designing algorithm-centric workflows, we often set our aims at scalability and performance as chief architectural goals. Equally often, making the design &lt;em&gt;extensible&lt;/em&gt; in new algorithms is overlooked or intentionally relegated as technical debt to be paid off in the future. This is quite natural, especially in the context of a startup: if you need to scale in order to survive, the need to add or remove algorithms easily in your system will arise when you’ve actually managed to survive long enough. It’s also true that having to rethink a design for scalability would often mean fundamental structural change, while making it easy to extend with new algorithms would usually only require changes in code deployed to the same structure; you’d rather work on your design’s muscles once its skeleton is stable than find yourself having to re-build its skeleton from scratch.&lt;/p&gt;

&lt;p&gt;However, if your product does depend on supporting new algorithms, you’re likely to eventually find that reducing the accidental complexity involved in adding or replacing them is a worthy goal. In this article, I’ll present a design pattern that might help you achieve that goal. Architectures vary greatly in nature and in the technologies involved in them, so there’s no silver bullet; but I might just have a certain kind of bullet to show you here, and it might be of assistance to you if you’re facing the right kind of werewolf.&lt;/p&gt;

&lt;p&gt;We’ll dive right in with an example and build up to a pattern from scratch. We’ll discuss the use case, the pattern and its different aspects in abstract terms.&lt;/p&gt;

&lt;h3 id=&quot;tldr&quot;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We design a basic worker that reads images from a queue and classifies them.&lt;/li&gt;
  &lt;li&gt;Upon wanting to support a new algorithm, we come up with a naïve extension.&lt;/li&gt;
  &lt;li&gt;We discuss the disadvantages it betrays in terms of extensibility. We figure out its main weakness is requiring &lt;em&gt;static mappings&lt;/em&gt; of supported algorithms.&lt;/li&gt;
  &lt;li&gt;We come up with an improved design that allows for &lt;em&gt;dynamic mappings&lt;/em&gt; of supported algorithms and thus makes the design easily extensible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;a-note-on-languages&quot;&gt;A note on languages&lt;/h3&gt;

&lt;p&gt;For the pattern to be reasonable enough to implement, it might require a few things of the programming language you use for algorithms. The main such requirement is that the language has some sort of mechanism for “reflection” or “inspection”, like the built-in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inspection&lt;/code&gt; module in Python. When this pattern started materializing itself in my work, Python was the only language I used for that, and so I can guarantee it will be very easy to code in Python. In any case, this it should be easy enough in any modern language.&lt;/p&gt;

&lt;p&gt;In parts II &amp;amp; III we’ll look at a concrete Python implementation for everything presented here.&lt;/p&gt;

&lt;h2 id=&quot;the-case&quot;&gt;The case&lt;/h2&gt;

&lt;p&gt;Let’s say we’re working on a message board platform that supports posting images. As it turns out, many of the images people post on our platform are memes. Upon realizing this, the product team for this platform decide they want to better understand the usage of memes in our board, and the need eventually arises of deploying a &lt;em&gt;meme classifier&lt;/em&gt;. This would be an algorithm that, given a meme image, tells us which among a predefined list of memes of interest the input image is most likely using.&lt;/p&gt;

&lt;p&gt;So, imagine an ML team develops an algorithm. At the same time, our back-end engineers set up a &lt;em&gt;queue&lt;/em&gt; from which images are to be read for meme classification. This requires us to create some kind of workflow where images are consumed from the queue and fed to the classifier. As ML Ops engineers in charge of deploying the meme classifier quickly, this might prompt us to develop a basic analysis worker with a controller component that reads from the queue and a runner component housing the classifier code.&lt;/p&gt;

&lt;h2 id=&quot;basic-worker&quot;&gt;Basic worker&lt;/h2&gt;

&lt;p&gt;A common practice is to deploy algorithms and worker code in general into container environments that can be replicated and scaled up and down to fit demand. I tend to favor containerizing all componentes that go into a worker setup as this lends itself very well to using orchestration tools such as AWS ECS or Kubernetes. It also helps minimize differences between development, staging and production environments.&lt;/p&gt;

&lt;p&gt;We’ll create a &lt;em&gt;controller&lt;/em&gt; component responsible for reading messages from the queue and sending results. This component will feed inputs from queued messages to a second &lt;em&gt;runner&lt;/em&gt; component, responsible for running the actual classifier code.&lt;/p&gt;

&lt;p&gt;We can use a local network for our componentes to communicate with each other, which is a good practice encouraged by Docker and by multi-container application tools such as Docker Compose. In our particular case, for the controller/runner communication, a simple client/server pattern over HTTP looks like a natural way to go: as the controller reads messages, it POSTs inputs to the runner and receives algorithm results. The only additional requirement this implies is for the runner to be packaged with an HTTP server in front of it that listens locally for requests by the controller. We use the “S” inside the runner component to denote the auxiliary server application it’s packaged with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/plug-play-worker-pattern-part-1/Untitled.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 30%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With both the controller and runner components containerized and communicating locally over HTTP, this design is sufficient to satisfy our requirements so far.&lt;/p&gt;

&lt;h2 id=&quot;the-need-to-extend&quot;&gt;The need to extend&lt;/h2&gt;

&lt;p&gt;Our design works well. After a few weeks of meme classification and analyzing stats, our platform’s product team realized that merely knowing the underlying memes used in posts is not sufficient, and that they also need to actually know the text it was overlaid with in each case. This would allow them to get insights into content posted in image form and analyze it for topic, sentiment and more. So, a new algorithm requirement in the form of OCR comes in.&lt;/p&gt;

&lt;p&gt;Sooner or later, the need to extend makes itself known.&lt;/p&gt;

&lt;h3 id=&quot;re-designing-for-extensibility&quot;&gt;Re-designing for extensibility&lt;/h3&gt;

&lt;p&gt;Looking to add the new OCR algorithm to the same worker setup, there are a number of different approaches we could take. In particular, these approaches may differ in whether they lead us to deploy new algorithms into a single “runner” container or new ones. While these approaches merit their own analysis and discussion, ending up in need of adding and removing runner containers in our worker setup is more than plausible in real-world applications. As long as you believe this might be true for you, having a design that allows for the easy addition, removal and update of algorithms either in existing runner containers or in new ones can prove very useful in the long run. This is what we’ll focus on here, and by the end we’ll have a low-overhead design that is highly extensible in new runner containers and easy to replicate.&lt;/p&gt;

&lt;h3 id=&quot;naïve-extension&quot;&gt;Naïve extension&lt;/h3&gt;

&lt;p&gt;Say we are now delivered the new OCR code for us to deploy. At this stage, if we were to deploy the OCR code into our single, preexisting runner container, our job would be simple enough: have the auxiliary server application know how to invoke the new code similarly to how it’s been doing the meme classifier, re-build, re-deploy and we’re done. The case where we want to deploy it into a new container, however, is a bit more interesting and more readily reveals the pitfalls in our initial design.&lt;/p&gt;

&lt;p&gt;So, let’s just assume that our OCR lives in a new project, coded and versioned separately, and we add some appropriate build logic so we can deploy it in good containerly fashion. The “easy” and naïve way to now add this new runner container to our worker would be to just throw in there and hook it up to the controller via HTTP on a new port.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/plug-play-worker-pattern-part-1/Untitled%201.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This would require we set up our new container with an auxiliary HTTP server with an endpoint for the controller to request OCR, similarly to how we did before with our meme classifier. The best way to do this would probably be to code up a single server application that knows how to listen to requests for both algorithms and to deploy it with both runner containers. It would also require we have the controller know port Y as well as X, and associate meme classification with the latter and OCR with the former.&lt;/p&gt;

&lt;p&gt;Thinking ahead, this naïve strategy would necessitate that both our controller and server application have &lt;em&gt;static mappings&lt;/em&gt; of all supported algorithms. This means that, each time we add an algorithm to our environment, new builds or re-configurations have to be made and deployed of the controller as well as the corresponding runner with updates to those mappings. Even worse: an update to the server application’s static mappings (as well as any other kind of update to its code) would entail either re-building &lt;em&gt;all&lt;/em&gt; runner images (as all are built with the same server application in front) or, to avoid re-building all of them, having our CI/CD process statically map know our algorithms too so it can map algorithm-to-image and selectively build affected images only in each case; our CI/CD workflow would then be equally affected with each update to our algorithm repertoir. Note that, even if we forego this mitigation through CI/CD knowing our algorithms on a first-name basis, we still have to dote it with a way to build the single server application into all runner images, e.g by using Docker multi-stage builds, which is a an additional challenge of its own. Also note that another consequence of these static mappings is that, once we have several runner containers, adding new algorithms to preexisting runners is no longer as trivial either.&lt;/p&gt;

&lt;h3 id=&quot;the-goal&quot;&gt;The goal&lt;/h3&gt;

&lt;p&gt;In looking at this breakdown of disadvantages for our initial approach, we may note that the &lt;em&gt;static&lt;/em&gt; nature of the &lt;strong&gt;algorithm-to-port&lt;/strong&gt; mappings in the controller and the &lt;strong&gt;algorithm-to-code&lt;/strong&gt; mappings in the auxiliary server is at the core of this design’s shortcomings, as this static nature is what produces the need to re-build everything each time we make algorithm-wise updates to our worker environment. With this in mind, our goal becomes clear: to find a way to make &lt;strong&gt;container ports&lt;/strong&gt; and &lt;strong&gt;algorithm code&lt;/strong&gt; &lt;em&gt;dynamically mappable&lt;/em&gt; or &lt;em&gt;discoverable&lt;/em&gt;. If we achieve that, then adding or removing algorithms becomes much simpler automatically: a new algorithm in an existing container requires re-building its image and its image alone; a new algorithm in a new container is discovered by the controller automatically.&lt;/p&gt;

&lt;p&gt;Sounds too good to be true, right? So, what’s the catch?&lt;/p&gt;

&lt;h3 id=&quot;the-pattern&quot;&gt;The pattern&lt;/h3&gt;

&lt;p&gt;The idea for the pattern is simple, and natural if we look at through a microservice lens: if we need dynamic discovery of runners and algorithms, then let’s add a component for “dynamic runner discovery”. This Runner Discovery component will be a component that holds, after initialization, an in-RAM mapping of every algorithm supported in that worker environment to the port of the container that can run it. Provided every algorithm can only run in one container, which is a sensible precondition, this mapping is all the information the controller will need to execute its task. The controller will request this mapping from the discovery component and automatically know what port it can find each algorithm on. So there’s a bit of initialization code to add to the controller here that will request the mappings from this new component.&lt;/p&gt;

&lt;p&gt;The remaining question is: how does Runner Discovery get its hands on such a mapping? Well, there’s another bit of initialization code to be added, this time to the runner containers. Upon initialization, each runner would have to come up with a list of the algorithms it supports, the port it is running on and send all of this info to Runner Discovery. This demands that the containers are able to do some discovery of their own to get their list of algorithms dynamically. Right off the bat, this sounds like it should be possible with any language that supports some sort of reflection that would allow one to discover implemented functions or modules in a container’s application code. As noted in the introduction, each language would require us to think of a way to do this reasonably; but since we’re sticking to an at-least-Python constraint here, we’ll be perfectly good to go in this regard.&lt;/p&gt;

&lt;p&gt;This is how applying these ideas to our setup would look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/plug-play-worker-pattern-part-1/Untitled%202.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 120%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note too that this pattern constrains the initialization order of components. Runner Discovery has to start first so that the runners can then register with it. In turn, the controller can only start once all runners are done registering. In summary:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Runner Discovery initializes.&lt;/li&gt;
  &lt;li&gt;Runners initialize, POST list of supported algorithms and port to Runner Discovery.&lt;/li&gt;
  &lt;li&gt;Controller initializes, GETs algorithm-to-port mapping from Runner Discovery.&lt;/li&gt;
  &lt;li&gt;Controller sends algorithm requests to runners until work is done.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For reference, this what the general case might look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/plug-play-worker-pattern-part-1/Untitled%203.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, herein lies the catch: we add a discovery component to our setup and a bit of code to the controller and runners to interact with the new discovery component. The modified controller is represented as “Controller*” in the diagram, and the “D” in the runners denotes the new discovery-related code. This is to represent the fact that we’d want to single-source that functionality and package it in each runner build, much the same way we’d do with the auxiliary server code.&lt;/p&gt;

&lt;p&gt;I submit, however, that this is a small price to pay, and it’s the offloading of mapping responsibility to this new component that gives us the discussed benefits of dynamic algorithm mapping. It’s true that those bits of added code need to be implemented and maintained, but they’re simple to implement and can be simple enough to maintain as well. First and foremost, they remain constant as a function of algorithm addition or removal, as they must be algorithm-agnostic by their very nature of providing dynamic mapping of algorithms. Furthermore, the controller’s bit which queries Runner Discovery can be added to and versioned with the very same controller code (thereby yielding *Controller**), which means it lives in and affects that single source project alone.&lt;/p&gt;

&lt;p&gt;If we figure out a way to also single-source the runner bit of discovery-related initialization code, our goal of making our design low-overhead when extending with new algorithms will be achieved. This is certainly possible by taking a multi-stage build kind of approach as mentioned before for the auxiliary server, only lower in overhead in this case due to it not requiring updates and re-builds with each change to the environment’s repertoir of algorithms. &lt;strong&gt;Spoiler alert&lt;/strong&gt;: it can also be made a lot easier by just coding both the “S” and “D” logic as a standalone Python package that simply discovers its runner code in a specified path in the filesystem and uses inspection to expose its algorithms on an HTTP server. This is what we’ll do in Part III.&lt;/p&gt;</content><author><name></name></author><category term="ml-ops" /><category term="tech" /><summary type="html">Note</summary></entry><entry><title type="html">ML Ops: A Fictional Case Study</title><link href="/2021/12/04/ml-ops-fict-1.html" rel="alternate" type="text/html" title="ML Ops: A Fictional Case Study" /><published>2021-12-04T12:14:00-03:00</published><updated>2021-12-04T12:14:00-03:00</updated><id>/2021/12/04/ml-ops-fict-1</id><content type="html" xml:base="/2021/12/04/ml-ops-fict-1.html">&lt;p&gt;In this article, I’d like to walk you through how the incorporation of ML Ops practices might be of great assistance to an engineering team looking to apply ML to solve a problem.&lt;/p&gt;

&lt;h2 id=&quot;the-case&quot;&gt;The case&lt;/h2&gt;

&lt;p&gt;So, meet Riley, an engineer at &lt;em&gt;Nicr&lt;/em&gt;. Nicr (as in “nicer”, but hipper) is a small startup developing a new model of interaction in social networks. Of course, I just made both Riley and nicr up completely. But bear with me; it will be worth it. Plus, you might just end up with an inspiring idea for a brand new social network after reading this article! That can’t be too bad now, can it?&lt;/p&gt;

&lt;p&gt;OK, don’t quote me on that last part.&lt;/p&gt;

&lt;h3 id=&quot;the-product&quot;&gt;The product&lt;/h3&gt;

&lt;p&gt;We’ll keep it simple. Nicr has built an online social platform where users can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create posts&lt;/li&gt;
  &lt;li&gt;Comment on posts&lt;/li&gt;
  &lt;li&gt;Up-vote or down-vote posts and comments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But there’s a twist: as a user is about to submit their post or comment, the product team at Nicr wants them to recieve an automatically generated feedback scoring the “niceness” of their text in the form of a a numerical score from 0 to 10. The user will then be able to decide whether to post their content as is, modify it, delete it or ignore the niceness feedback altogether. The product team at Nicr has dubbed this the &lt;em&gt;nice-a-meter&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;They’re quite excited about it.&lt;/p&gt;

&lt;h3 id=&quot;the-implementation&quot;&gt;The implementation&lt;/h3&gt;

&lt;p&gt;Remember Riley? Tasked with implementing this spceial feature, Riley is quick to conclude her chances of success lie in the realm of Machine Learning. Being an experienced engineer and having always held an ear to the ground of tech, she knew well enough that &lt;em&gt;sentiment analysis&lt;/em&gt; was a thing. Knowing how far ML has come and having played around with some of it on her own, she was also pretty sure she’d be able to easily find some tools and information to help her out. Indeed, a few queries to her favorite search engines yielded a slew of promising tutorials, articles and libraries.&lt;/p&gt;

&lt;p&gt;Soon thereafter, having gone through &lt;a href=&quot;https://huggingface.co/transformers/quicktour.html&quot;&gt;Hugging Face’s great quick tour&lt;/a&gt; and a visit or two to Stack Overflow, Riley has a working snippet of code that’s able to process an arbitrary string and return a sentiment score. To hook it up, she deploys a server application with this code and its dependencies to her IaaS of choice and exposes it through a simple HTTP API offering a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST /sentiment-analysis&lt;/code&gt; endpoint. Her client apps asynchronously send their requests to this server for each comment or post about to be submitted. Eventual issues of scale would be managed by creating a server image, scaling it horizontally and load balancing.&lt;/p&gt;

&lt;p&gt;So far so good.&lt;/p&gt;

&lt;!--[note futuro challenge de Nicr podría ser deployar modelo a celu si por ejemplo empiezan a soportar feedback en vivo con cada palabra que se tipea -&gt; considerar si Palanca ofrece edge deployment tipo OctoML].--&gt;

&lt;h3 id=&quot;the-problem&quot;&gt;The problem&lt;/h3&gt;

&lt;p&gt;Nicr’s social network grows, its teams grow and the platform gains a healthy base of loyal users. Problems arise, as they always do and always will; but they’re swiftly curbed by Nicr’s teams filled with fresh energy and motivation.&lt;/p&gt;

&lt;p&gt;At a certain point, however, one particularly pernitious problem starts to make itself known. With conspicuously increasing frequency, user-submitted reports are coming in of unexpected nice-a-meter results on their content. Riley, now head of Nicr’s engineering team, takes a look at reported cases to get a sense of the issue. With an idea in mind of what could be going wrong, she and her team sit down to have it out with the nice-a-meter themselves. Things do seem a bit off, and soon enough they form a solid hypothesis of the root of the problem.&lt;/p&gt;

&lt;p&gt;As Nicr’s platform grew and its community became tighter, so did the language employed by its users evolve. As this happened, new and idiosyncratic ways of conveying sentiment emerged and flourished. Riley’s initial Hugging Face out-of-the-box model, however, stayed back in time: it had never “seen” this language in its training, with new expressions coming up that could not have been reasonably associated with their new-found meaning and new words previously unused in human interaction. In other words, Riley’s ML model had caught a strong case of &lt;em&gt;model drift&lt;/em&gt;: it had stagnated, its current training clearly outdated with respect to the evolving nature of the data it’s employed to process.&lt;/p&gt;

&lt;h3 id=&quot;an-initial-solution&quot;&gt;An initial solution&lt;/h3&gt;

&lt;p&gt;Broad strokes: if a model is outdated, then an update is in order. A few questions therefore immediately come to mind&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;How might we train our model on our own data?&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;How might we validate the new model is better than the old one?&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having been in charge of shipping the nice-a-meter initially, Riley takes on this task. Digging back into search engines Hugging Face’s Transformers docs, she learns that their model can be fine-tuned with custom datasets, and that once such a dataset is ready it could also be used to validate the new model’s performance by running it on train/test splits or cross validation. In need of a dataset &lt;!--[note dataset feature]--&gt;, Riley mines the logs for samples that include interesting cases of mis-classification &lt;!--[note &quot;no siempre va a ser tan fácil&quot; e.g mucha data o imágenes]--&gt; to be manually supervised, &lt;!--[note ordering by prob]--&gt; and tagging work is divided among the team &lt;!--[note &quot;no siempre se va a poder hacer&quot;, dividir la data entre muchos tampoco es fácil, le quita mucho tiempo a todos]--&gt;.&lt;/p&gt;

&lt;p&gt;As for the fine-tuning code, it becomes clear to Riley that this will not be as breezy as setting up the out-of-the-box model was; a decent amount more of research and trial-and-error is required. But, before long, her fine-tuning code is ready to go. With the dataset all cooked up, a new model is trained and to encouraging results: running some metrics on the test split using the old model and the new one shows a significant improvement, validating the team’s suspicions and their efforts towards a fix. The new model is deployed in much the same way that the first one was, with Riley manually uploading it to a server’s disk, creating a new image and rolling it out using her cloud computing service’s general-purpose deployment functionalities.&lt;/p&gt;

&lt;h3 id=&quot;what-about-the-next-time&quot;&gt;What about the next time?&lt;/h3&gt;

&lt;p&gt;Nevertheless, a third and important question had also come to Riley’s mind when she understood what had happened:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;What about the next time?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just as the nature of data in the wild had changed enough at that point to cause drift, so it is bound to do again. Would they just wait for user reports to pour in again and repeat the exact same process? They’d have to manually prepare a dataset, spend time dividing and managing supervision work, manually re-train and manually re-deploy. Monitoring will be left again to an indirect means such as mining subsequent user-submitted reports. Whenever this problem arises again, they’d have to scramble to figure out what’s causing the new drift before even having a chance at working on fixing it. Needless to say, all the while they’d be allowing a degradation of service to take place that they might not be able to afford, and crucial work hours would be allocated away from other important bugs and features.&lt;/p&gt;

&lt;p&gt;And right as Riley is sitting at her desk pondering these worrying questions, a Slack message from the product team makes its way to her screen: &lt;em&gt;Hey Riley! Listen, looks like spam is starting to become a problem. I’ve heard you can use Machine Learning for that?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-about-new-algorithms&quot;&gt;What about new algorithms?&lt;/h3&gt;

&lt;p&gt;So yes, Machine Learning is probably the best solution for spam classification. Any data scientist’s introductory “hello world” classifier, it’s also pretty much a solved problem. However, spam may be idiosyncratic to a platform as well, and if Nicr’s exeprience with sentiment analysis is any indication, it will become so more and more with time. To make things worse, it now appears that spammers have been using images to post their spam on Nicr, with obfuscated text in images and watermarked logos of sketchy online businesses. So, much of what they had done for their sentiment analysis problem would not be of use, having new challenges in setting up ways to tag images and handle image datasets, manage performance and hardware issues related to running ML on images and more. Should they invest time and resources in creating and standardizing processes to monitor, re-train and deploy models?&lt;/p&gt;

&lt;p&gt;It couldn’t be any clearer to Riley that a better solution is needed.&lt;/p&gt;

&lt;h2 id=&quot;enter-ml-ops&quot;&gt;Enter ML Ops&lt;/h2&gt;

&lt;p&gt;Let’s take a step back and have a look at the bigger picture. Riley and Nicr’s fictional case makes the following point: things get really messy as the use of Machine Learning is extended in application and in time within an organization. Scaling ML models, keeping training data up-to-date and ensuring models are in tip-top shape is hard, and becomes harder as the number of models increase and the kinds of problems they’re applied to diversifies.&lt;/p&gt;

&lt;p&gt;That’s where ML Ops, or Machine Learning Operations, come in. Think about it this way: nowadays, you wouldn’t dream of scaling a software development effort without practicing proper DevOps. Just as with DevOps for development, a means of standardizing processes and incorporating reliable and flexible automation is the key to success in scaling with Machine Learning.&lt;/p&gt;

&lt;!--[## _Palanca_ and the Next Chapter]--&gt;

&lt;!--[With this observation in mind, the idea for [_Palanca_](https://www.gopalanca.com/) was born. At Palanca, we're committed to providing ML Ops solutions for business looking to do exactly that. As well as offering tailored solutions for clients, we're also hard at work in crafting a powerful, single-dashboard tool that strives to concentrate everything a team might need to step up their ML Ops immediately by integrating it into their workflow.]--&gt;

&lt;!--[Bringing our prior experience in the field to the table and levaraging the latest tools and infrastructure technology, whether by custom-made solutions or through a powerful dashboard, we're eager to help businesses maximize their ML potential by facing the kinds of challenges exemplified here, and more, successfully.]--&gt;

&lt;!--[So, as is probably clear by now, we're pretty confident we'd have been of great help to Riley and the Nicr team. Given the issues they encountered in their application of ML to their product, they would have benefited greatly from bringing ML Ops practices into their development process.]--&gt;

&lt;!--[In a following article, we'll take a good look at how this might happen. We'll present ML Ops practices and some of the most exciting features of our work-in-progress tool by examining in detail how Riley and her team might have levaraged them to great advantage.]--&gt;

&lt;!--[Entonces, &quot;let's step back&quot;. Riley necesita abstracciones, procesos, automatización etc -&gt; Palanca ofrece eso -&gt; Veamos como Riley podría aprovechar nuestros productos -&gt; Riley entra acá, toca acá, sube allá, monitorea acullá. [note screenshots]]--&gt;
&lt;!--[Ahora que cuentan con eso, en Nicr de repente pueden empezar a aplicar ML para resolver cosas mucho más interesantes y complejas (e.g predicción de up-votes/down-votes en función del sentimiento del contenido, predicción de churn en base a up-votes/down-votes o sentimiento de comentarios en sus posts. Noter que estos algoritmos tendrían inputs &quot;compuetsos&quot; and not just texto o imagen y nosotros ofreceríamos funcionalidad para eso).]--&gt;
&lt;!--[Encontrar la manera de mencionar otros features que se desprenden de tener algo así como el poder trackear historial de modelos (para e.g eventualmente decidirse por cambio de arquitectura de red), versionado y rollbacks, sandboxing para experimentar etc, cálculo de métricas subseleccionando features de la data.]--&gt;
&lt;!--[Sugerir al final somehow que el producto no está terminado y veríamos qué necesidades tiene cada cliente?]--&gt;
&lt;!--[Que quede claro que el punto es que las necesidades específicas o idiosincráticas no tardan en aparecer ni tampoco lo hace la multiplicidad de modelos.]--&gt;
&lt;!--[btw, in case you were wondering, Riley is now CTO porque la promocionaron por tomar buenas decision como contratarnos :P]--&gt;</content><author><name></name></author><category term="ml-ops" /><category term="tech" /><summary type="html">In this article, I’d like to walk you through how the incorporation of ML Ops practices might be of great assistance to an engineering team looking to apply ML to solve a problem.</summary></entry></feed>